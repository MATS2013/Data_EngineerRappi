# DATA_ENGINEEREAPPI

### Proyecto de MigraciÃ³n, TransformaciÃ³n y ValidaciÃ³n de Datos con Apache Airflow y SQLite

Este proyecto implementa un flujo **ETL (Extract, Transform, Load)** usando **Apache Airflow** en contenedores **Docker**, con base en datos locales en formato CSV y una base de datos **SQLite** como referencia.  
Permite automatizar la creaciÃ³n de la base de datos, la transformaciÃ³n de registros contables, la validaciÃ³n de saldos y la generaciÃ³n de reportes de calidad de datos.

---

## Estructura del Proyecto

```
DATA_ENGINEEREAPPI/
â”‚
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ data_migration_dag.py        # DefiniciÃ³n del DAG principal de Airflow
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ processed/                   # Resultados procesados por el DAG
â”‚   â”‚   â”œâ”€â”€ account_summary.csv
â”‚   â”‚   â”œâ”€â”€ invalid_transactions.csv
â”‚   â”‚   â”œâ”€â”€ transformed.csv
â”‚   â”‚   â””â”€â”€ report.txt               # Reporte final generado automÃ¡ticamente
â”‚   â””â”€â”€ reference/                   # Datos base de entrada
â”‚       â”œâ”€â”€ accounts.csv
â”‚       â”œâ”€â”€ journal_entries.csv
â”‚       â””â”€â”€ local.db
â”‚
â”œâ”€â”€ logs/                            # Logs generados por Airflow
â”œâ”€â”€ plugins/                         # Carpeta reservada para plugins personalizados
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ connection/
â”‚   â”‚   â”œâ”€â”€ data_base.py             # Crea la base local desde los CSV
â”‚   â”‚   â””â”€â”€ sql_engine.py            # Maneja la conexiÃ³n y operaciones con SQLite
â”‚   â”‚
â”‚   â”œâ”€â”€ constants/
â”‚   â”‚   â”œâ”€â”€ paths.py                 # Control de rutas
â”‚   â”‚   â””â”€â”€ queries.py               # Consultas SQL base del proceso ETL
â”‚   â”‚
â”‚   â””â”€â”€ work_flows/
â”‚       â””â”€â”€ data_migration_flow.py   # LÃ³gica ETL: transformaciÃ³n, validaciÃ³n y reporte
â”‚
â”œâ”€â”€ .env                             # Variables de entorno para Airflow
â”œâ”€â”€ docker-compose.yaml              # OrquestaciÃ³n de contenedores
â”œâ”€â”€ requirements.txt                 # Dependencias del entorno
â””â”€â”€ README.md                        # DocumentaciÃ³n del proyecto
```

---

## Funcionalidad del DAG `data_migration_dag.py`

El DAG **`data_migration_dag`** define un flujo completo de validaciÃ³n de datos:

1. **ensure_database_exists** 
   Crea la base de datos `local.db` si no existe, cargando los CSV de referencia.

2. **transform_records** 
   Ejecuta la transformaciÃ³n de datos contables (JOIN + normalizaciÃ³n) y guarda el resultado en `transformed.csv`.

3. **validate_balances** 
   Detecta transacciones con desbalances (dÃ©bito â‰  crÃ©dito) y guarda los resultados en `validation.csv`.

4. **summary_accounts** 
   Genera un resumen agregado por cuenta contable.

5. **generate_report** 
   Filtra las transacciones vÃ¡lidas (`is_valid_transaction == 1`), elimina la columna de validaciÃ³n y genera el reporte final `report.txt`.  
   Si el porcentaje de registros invÃ¡lidos supera el 5%, el DAG lanza una excepciÃ³n para marcar fallo de calidad.

---

## Despliegue con Docker + Airflow

El proyecto estÃ¡ completamente **contenedorizado**, utilizando tres servicios principales definidos en `docker-compose.yaml`:

| Servicio              | FunciÃ³n principal |
|------------------------|------------------|
| **airflow-postgres**   | Base de datos de metadatos para Airflow |
| **airflow-webserver**  | Interfaz web accesible desde `localhost:8080` |
| **airflow-scheduler**  | Ejecuta y programa las tareas del DAG |
| **airflow-init**       | Inicializa la base de datos de Airflow y crea el usuario admin |

---

## Pasos para la EjecuciÃ³n

### Clonar el repositorio
```bash
git clone https://github.com/<usuario>/DATA_ENGINEEREAPPI.git
cd DATA_ENGINEEREAPPI
```

### Requisitos Previos

1. **Docker y Docker Compose** instalados en el sistema.
2. **Python 3.11 o 3.12** si se desea ejecutar las funciones fuera de contenedor.
3. Acceso a PowerShell o terminal equivalente.

---

### ConfiguraciÃ³n del Entorno Docker

El archivo `docker-compose.yaml` define los servicios necesarios para ejecutar Apache Airflow de manera local.  
Los servicios principales son:

- **airflow-webserver:** Interfaz web de Airflow (puerto 8080).
- **airflow-scheduler:** Programa que gestiona la ejecuciÃ³n del DAG.
- **airflow-postgres:** Base de datos que almacena la configuraciÃ³n y el historial de Airflow.

### Contenido del `docker-compose.yaml` (resumen)

```yaml
services:
  airflow-webserver:
    image: apache/airflow:2.6.3
    ports:
      - "8080:8080"
  airflow-scheduler:
    image: apache/airflow:2.6.3
  airflow-postgres:
    image: postgres:13
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
```

---

## Instrucciones de EjecuciÃ³n

AsegÃºrate de abrir **PowerShell** y ubicarte en la carpeta raÃ­z del proyecto.  
Por ejemplo:

```bash
cd "D:\Pruebas tecnicas\data_engineer_rappi\Data_EngineerRappi"
```

Luego, sigue los pasos en orden:

### 1. Descargar las imÃ¡genes necesarias
```bash
docker compose pull
```

### 2. Crear y ejecutar el contenedor principal
```bash
docker compose up airflow-webserver --force-recreate --no-deps -d
```

### 3. Detener y eliminar volÃºmenes anteriores
```bash
docker compose down --volumes --remove-orphans
```

### 4. Iniciar el servicio de base de datos (Postgres)
```bash
docker compose up -d airflow-postgres
```

### 5. Inicializar la base de datos interna de Airflow
```bash
docker compose run airflow-webserver airflow db init
```

### 6. Levantar todos los servicios
```bash
docker compose up -d
```

### 7. Verificar que los contenedores estÃ©n activos
```bash
docker compose ps
```

## Crear y configurar las variables de entorno
Edita el archivo `.env` con tus datos si es necesario:
```bash
_AIRFLOW_WWW_USER_USERNAME=admin
_AIRFLOW_WWW_USER_PASSWORD=admin
```

## Acceder a la interfaz de Airflow
Abre el navegador en:

```
http://localhost:8080
```

Usuario: **airflow**  
ContraseÃ±a: **airflow**

---

## ğŸª¶ EjecuciÃ³n del DAG

1. En la interfaz de Airflow, busca el DAG llamado **`data_migration_dag`**.  
2. ActÃ­valo moviendo el switch a **ON**.  
3. Haz clic en el botÃ³n â–¶ï¸ **Trigger DAG** para ejecutarlo.  
4. Observa el flujo en el **Graph View** o revisa los logs de cada tarea.

---

## Salidas del Proceso

Una vez completado el flujo, los archivos resultantes estarÃ¡n en:

```
data/processed/
```

| Archivo | DescripciÃ³n |
|----------|-------------|
| `transformed.csv` | Datos transformados con validaciÃ³n de campos |
| `validation.csv`  | Transacciones con desbalances |
| `summary.csv`     | Resumen agregado por cuenta |
| `report.txt`      | Reporte final de calidad de datos |

## Ejemplo de contenido del `report.txt`
```
=== Data Migration Valid Transactions Report ===

Total records processed: 245
Valid records after filtering: 240
Invalid percentage: 2.04%

=== Transformed Transactions ===
transaction_id  transaction_date  account_number  account_name  debit_amount  credit_amount
1               2024-01-10        1001            Caja          500.00        0.00
...

=== Validation Transactions ===
transaction_id  total_debits  total_credits
102             1500.00       0.00
...
```

---

## Dependencias Principales

- **Python 3.10+**
- **Apache Airflow 2.6.3**
- **Pandas**
- **SQLite3**
- **PostgreSQL 13 (para Airflow metastore)**
- **Docker y Docker Compose**

InstalaciÃ³n local (opcional, fuera del contenedor):
```bash
pip install -r requirements.txt
```

---

## Variables de Entorno Importantes

Definidas en `.env`:

| Variable | DescripciÃ³n |
|-----------|-------------|
| `_AIRFLOW_WWW_USER_USERNAME` | Usuario del panel web |
| `_AIRFLOW_WWW_USER_PASSWORD` | ContraseÃ±a del usuario |
| `_AIRFLOW_WWW_USER_EMAIL` | Correo asociado |
| `PROJECT_HOME` | Directorio raÃ­z de Airflow dentro del contenedor |
| `PYTHONPATH` | Rutas a mÃ³dulos fuente |

---

## Flujo de LÃ³gica (Resumen)

```mermaid
flowchart TD
    A[Start DAG] --> B[ensure_database_exists]
    B --> C[transform_records]
    C --> D[validate_balances]
    D --> E[summary_accounts]
    E --> F[generate_report]
    F --> G[Report & CSV outputs saved]
```

---

## Mantenimiento y ExtensiÃ³n

- Puedes aÃ±adir nuevos DAGs en la carpeta `dags/`.
- Los scripts del flujo principal se ubican en `src/work_flows/`.
- Si cambias los nombres de las tablas o CSV, ajusta las consultas SQL en `src/constants/queries.py`.
- Los logs de ejecuciÃ³n se encuentran en la carpeta `logs/`.

---

## Autor

**Miguel Triana**  
Ingeniero MecatrÃ³nico | Especialista en Gerencia Financiera  
AutomatizaciÃ³n, Python, SQL y Ciencia de Datos  
ğŸ“§ samuellulu200021@gmail.com  
ğŸŒ [LinkedIn](https://www.linkedin.com/in/miguel-triana-993b0925a/)
